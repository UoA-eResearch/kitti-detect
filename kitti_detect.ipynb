{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automated-convenience",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to: https://towardsdatascience.com/building-your-own-object-detector-pytorch-vs-tensorflow-and-how-to-even-get-started-1d314691d4ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycocotools\n",
    "from PIL import Image, ExifTags\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "from glob import glob\n",
    "from skimage import transform\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITTIDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, label_dir, label_map, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.img_paths = glob(os.path.join(img_dir, \"*.jpg\"))  \n",
    "        self.label_map = label_map        \n",
    "                \n",
    "    def parse_kitti(self, path):\n",
    "        # https://github.com/NVIDIA/DIGITS/blob/v4.0.0-rc.3/digits/extensions/data/objectDetection/README.md\n",
    "        objects = []\n",
    "        with open(path, 'r') as f:\n",
    "            for line in [l.strip() for l in f.readlines()]:\n",
    "                label, _, _, _, xmin, ymin, xmax, ymax, *_ = line.split()\n",
    "                objects.append({'label': label, 'bounds': [float(x) for x in [xmin, ymin, xmax, ymax]]})            \n",
    "        return objects            \n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # load images and bounding boxes        \n",
    "        img_path = self.img_paths[idx]\n",
    "        img_id, _ = os.path.splitext(os.path.basename(img_path))        \n",
    "        label_path = os.path.join(self.label_dir, img_id + \".txt\")\n",
    "                \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "                        \n",
    "        objects = self.parse_kitti(label_path)                \n",
    "        boxes = torch.tensor([o['bounds'] for o in objects], dtype=torch.float32)\n",
    "        labels = torch.tensor([self.label_map[o['label']] for o in objects], dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:,0])\n",
    "        target[\"iscrowd\"] = torch.zeros((len(objects),), dtype=torch.int64)\n",
    "        #target[\"filename\"] = img_path\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))        \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    # load an object detection model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # replace the pre-trained head with a new on\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_annotation(image, target, label_map, prediction=None, thresh=0, size=None, title=\"\"):\n",
    "    \n",
    "    # invert labelmap\n",
    "    label_map = {v:k for k,v in label_map.items()}\n",
    "    \n",
    "    label_offset_x = 0\n",
    "    label_offset_y = -2\n",
    "    fig, ax = plt.subplots(figsize=size)   \n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # image resize\n",
    "    #width, height, channels = image.shape\n",
    "    #image = transform.resize(image, (2*width, 2*height))\n",
    "    \n",
    "    ax.imshow(image) # assumes image is a torch.tensor\n",
    "    \n",
    "    # ground truth\n",
    "    boxes = target['boxes']    \n",
    "    for i in range(boxes.size()[0]):\n",
    "        x1, y1, x2, y2 = target['boxes'][i]\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')    \n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1 + label_offset_x, y1 + label_offset_y, f\"{label_map[target['labels'][i].item()]}\", color='r')\n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "        \n",
    "    # prediction\n",
    "    if prediction:\n",
    "        boxes, scores, labels = prediction['boxes'], prediction['scores'], prediction['labels']\n",
    "        for i in range(boxes.size()[0]):            \n",
    "            if scores[i] > thresh:\n",
    "                x1, y1, x2, y2 = boxes[i]\n",
    "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='g', facecolor='none')    \n",
    "                ax.add_patch(rect)  \n",
    "                text = f\"{label_map[labels[i].item()]} {scores[i]:.2f}\"\n",
    "                ax.text(x1 + label_offset_x, y2 - 6*label_offset_y, text, color='g')\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-separate",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../KITTI_Test/combined/resized\" # these are the scaled images used for training\n",
    "image_dir = os.path.join(data_dir, \"images\")\n",
    "label_dir = os.path.join(data_dir, \"labels\")\n",
    "\n",
    "# need a default background class\n",
    "label_map = {\n",
    "    'background': 0,\n",
    "    'core': 1,\n",
    "    'flake': 2,\n",
    "    'flake_broken': 3,\n",
    "    'tool': 4\n",
    "}\n",
    "\n",
    "# define datasets\n",
    "ds = KITTIDataset(image_dir, label_dir, label_map, transforms=get_transforms(train=True))\n",
    "ds_test = KITTIDataset(image_dir, label_dir, label_map, transforms=get_transforms(train=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-premises",
   "metadata": {},
   "source": [
    "### Data Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-emphasis",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check example\n",
    "image, target = ds[88]\n",
    "display_annotation(image, target, label_map, size=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-collapse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check all images\n",
    "for i, path in enumerate(ds.img_paths):\n",
    "    image, target = ds[i]    \n",
    "    display_annotation(image, target, label_map, title=f\"[{i}]: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-disorder",
   "metadata": {},
   "source": [
    "### Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "n_test = 10\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(ds)).tolist()\n",
    "ds = torch.utils.data.Subset(ds, indices[:-n_test])\n",
    "ds_test = torch.utils.data.Subset(ds_test, indices[-n_test:])\n",
    "\n",
    "# define data loaders\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=4, shuffle=True, num_workers=1, collate_fn=utils.collate_fn)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=1, shuffle=False, num_workers=1, collate_fn=utils.collate_fn)\n",
    "print(f\"Total {len(indices)} samples, train: {len(ds)}, test: {len(ds_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-beauty",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-track",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # check all test images\n",
    "# for i in range(n_test):\n",
    "#     image, target = ds_test[i]\n",
    "#     display_annotation(image, target, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(len(label_map)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler which decreases the learning rate by # 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-display",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, dl, device, e, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "#     evaluate(model, dl_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-broadway",
   "metadata": {},
   "source": [
    "### Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-wagner",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "loaded_model = get_model(len(label_map))\n",
    "loaded_model.load_state_dict(torch.load(\"model.pt\"))\n",
    "loaded_model = loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 # this is the image number in the test set, 0 to n_test - 1\n",
    "image, target = ds_test[idx]\n",
    "reverse_label_map = {v:k for k,v in label_map.items()}\n",
    "\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = loaded_model([image.to(device)])    \n",
    "    display_annotation(image, target, label_map, prediction=pred[0], thresh=0.3, size=(10, 10))\n",
    "    labels, scores = pred[0]['labels'], pred[0]['scores']\n",
    "    for i in range(labels.size()[-1]):\n",
    "        print(reverse_label_map[labels[i].item()], \"--\", scores[i].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
