{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d9852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning: current data partitioning with label overrides assumes one object per image\n",
    "\n",
    "# credit to: https://towardsdatascience.com/building-your-own-object-detector-pytorch-vs-tensorflow-and-how-to-even-get-started-1d314691d4ae\n",
    "# for initial ideas\n",
    "\n",
    "# installing key packages:\n",
    "# conda install pytorch torchvision torchaudio cudatoolkit=11.0 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycocotools\n",
    "from PIL import Image, ExifTags\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "from glob import glob\n",
    "from skimage import transform\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from common import parse_kitti\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITTIDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, records, label_map, use_label_overrides=False, transforms=None):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        self.records = records\n",
    "        self.label_map = label_map\n",
    "        self.use_label_overrides = use_label_overrides\n",
    "        \n",
    "        if (self.use_label_overrides):\n",
    "            print(\"Warning, label overrides assume one object per image\")\n",
    "    \n",
    "    def get_image_path(self, idx):\n",
    "        return self.records[idx]['image_path']   \n",
    "    \n",
    "    def get_label_path(self, idx):\n",
    "        return self.records[idx]['label_path']   \n",
    "    \n",
    "    def get_label_override(self, idx):\n",
    "        return self.records[idx]['label_override']   \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        try:\n",
    "            # load images and bounding boxes        \n",
    "            image_path = self.get_image_path(idx)\n",
    "            label_path = self.get_label_path(idx)\n",
    "            label_override = self.get_label_override(idx)\n",
    "\n",
    "            img = Image.open(image_path).convert(\"RGB\")\n",
    "            objects = parse_kitti(label_path)                \n",
    "            boxes = torch.tensor([o['bounds'] for o in objects], dtype=torch.float32)\n",
    "            \n",
    "            # relabel if needed\n",
    "            if self.use_label_overrides:\n",
    "                labels = torch.tensor([self.label_map[label_override] for o in objects], dtype=torch.int64)\n",
    "            else:\n",
    "                labels = torch.tensor([self.label_map[o['label']] for o in objects], dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = labels\n",
    "            target[\"image_id\"] = torch.tensor([idx])\n",
    "            target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:,0])\n",
    "            target[\"iscrowd\"] = torch.zeros((len(objects),), dtype=torch.int64)            \n",
    "\n",
    "            if self.transforms is not None:\n",
    "                img, target = self.transforms(img, target)\n",
    "\n",
    "            return img, target\n",
    "        \n",
    "        except Exception as e:            \n",
    "            print(self.records[idx])\n",
    "            raise e\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))        \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes, pretrained=False):\n",
    "    # load an object detection model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # replace the pre-trained head with a new on\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_annotation(image, target, label_map, prediction=None, thresh=0, size=None, title=\"\"):\n",
    "    \n",
    "    # invert labelmap\n",
    "    label_map = {v:k for k,v in label_map.items()}\n",
    "    \n",
    "    label_offset_x = 0\n",
    "    label_offset_y = -2\n",
    "    fig, ax = plt.subplots(figsize=size)   \n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # image resize\n",
    "    #width, height, channels = image.shape\n",
    "    #image = transform.resize(image, (2*width, 2*height))\n",
    "    \n",
    "    ax.imshow(image) # assumes image is a torch.tensor\n",
    "    \n",
    "    # ground truth\n",
    "    boxes = target['boxes']    \n",
    "    for i in range(boxes.size()[0]):\n",
    "        x1, y1, x2, y2 = target['boxes'][i]\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')    \n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1 + label_offset_x, y1 + label_offset_y, f\"{label_map[target['labels'][i].item()]}\", color='r')\n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "        \n",
    "    # prediction\n",
    "    if prediction:\n",
    "        boxes, scores, labels = prediction['boxes'], prediction['scores'], prediction['labels']\n",
    "        for i in range(boxes.size()[0]):            \n",
    "            if scores[i] > thresh:\n",
    "                x1, y1, x2, y2 = boxes[i]\n",
    "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='g', facecolor='none')    \n",
    "                ax.add_patch(rect)  \n",
    "                text = f\"{label_map[labels[i].item()]} {scores[i]:.2f}\"\n",
    "                ax.text(x1 + label_offset_x, y2 - 6*label_offset_y, text, color='g')\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partitions(pattern):\n",
    "    parts = []\n",
    "    paths = glob(pattern)\n",
    "    for p in paths:\n",
    "        with open(p, 'r') as f:\n",
    "            parts.append(json.load(f))\n",
    "    return parts\n",
    "\n",
    "def train_test_partitions(parts, test_idx):\n",
    "    p_test = parts[test_idx]\n",
    "    p_train = []\n",
    "    for i in range(len(parts)):\n",
    "        if i != test_idx:\n",
    "            p_train += parts[i]\n",
    "    return p_train, p_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a618232b",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment name corresponds to the foldername and file prefix for data partitions and label maps\n",
    "exp_name = \"t1\"\n",
    "\n",
    "# load train and test data partitions\n",
    "pattern = f\"{exp_name}/{exp_name}-part-*.json\"\n",
    "partitions = load_partitions(pattern)\n",
    "test_fold = 0 # index of the test partition\n",
    "data_train, data_test = train_test_partitions(partitions, test_fold)\n",
    "print(f\"folds: {len(partitions)}, test_fold: {test_fold}, train samples: {len(data_train)}, test samples: {len(data_test)}\")\n",
    "\n",
    "# load the label map\n",
    "label_map_path = f\"{exp_name}/{exp_name}-label-map.json\"\n",
    "with open(label_map_path, 'r') as f:\n",
    "    label_map = json.load(f)\n",
    "print(f\"label map: {label_map}\")\n",
    "reverse_label_map = {v:k for k,v in label_map.items()}\n",
    "\n",
    "use_label_overrides = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c9e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train = data_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets and data loaders\n",
    "batch_size = 32\n",
    "ds = KITTIDataset(data_train, label_map, use_label_overrides=use_label_overrides, transforms=get_transforms(train=True))\n",
    "ds_test = KITTIDataset(data_test, label_map, use_label_overrides=use_label_overrides, transforms=get_transforms(train=False))\n",
    "\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=1, collate_fn=utils.collate_fn)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=1, shuffle=False, num_workers=1, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-premises",
   "metadata": {},
   "source": [
    "### Data Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check example training image\n",
    "image, target = ds[random.randrange(len(ds))]\n",
    "display_annotation(image, target, label_map, size=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-collapse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # check all images\n",
    "# for i, path in enumerate(ds.img_paths):\n",
    "#     image, target = ds[i]    \n",
    "#     display_annotation(image, target, label_map, title=f\"[{i}]: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9083552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check example test image\n",
    "image, target = ds_test[random.randrange(len(ds_test))]\n",
    "display_annotation(image, target, label_map, size=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a711a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check all test images\n",
    "# for i in range(n_test):\n",
    "#     image, target = ds_test[i]\n",
    "#     display_annotation(image, target, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-beauty",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#device = torch.device('cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(len(label_map)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "#optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "# and a learning rate scheduler which decreases the learning rate by # 10x every 3 epochs\n",
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-display",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "logs = []\n",
    "for e in range(epochs):    \n",
    "    logger = train_one_epoch(model, optimizer, dl, device, e, print_freq=10, grad_clip=1)\n",
    "    log_values = {k:{'median':v.median, 'mean':v.avg} for k,v in logger.meters.items()}\n",
    "    logs.append(log_values)    \n",
    "    # update the learning rate\n",
    "    #lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    #evaluate(model, dl_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9717972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save logs\n",
    "log_name = f\"{exp_name}/{exp_name}-model-{test_fold}-logs.json\"\n",
    "with open(log_name, 'w') as f:\n",
    "    json.dump(logs, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b13adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot log data\n",
    "def plot_logs(logs):    \n",
    "    median = []\n",
    "    mean = []\n",
    "    for i in range(len(logs)):        \n",
    "        median.append({k:v['median'] for k,v in logs[i].items()})\n",
    "        mean.append({k:v['mean'] for k,v in logs[i].items()})\n",
    "    \n",
    "    median = pd.DataFrame(median)\n",
    "    mean = pd.DataFrame(mean)\n",
    "    \n",
    "    nr = len(median.columns)\n",
    "    fig, axes = plt.subplots(nrows=nr, ncols=1, figsize=(9, 3*nr))\n",
    "    for i, name in enumerate(median.columns):\n",
    "        axes[i].set_title(name)\n",
    "        axes[i].set_xlabel('Epoch')\n",
    "        axes[i].set_ylabel('Loss')\n",
    "        axes[i].plot(median[name].values, label='median')\n",
    "        axes[i].plot(mean[name].values, label='mean')        \n",
    "        axes[i].legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6571a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_logs(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-broadway",
   "metadata": {},
   "source": [
    "### Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_path = f\"{exp_name}/{exp_name}-model-{test_fold}.pt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-wagner",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "loaded_model = get_model(len(label_map))\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "loaded_model = loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-england",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = random.randrange(len(ds_test))\n",
    "image, target = ds_test[idx]\n",
    "\n",
    "\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = loaded_model([image.to(device)])    \n",
    "    display_annotation(image, target, label_map, prediction=pred[0], thresh=0.8, size=(10, 10))\n",
    "    labels, scores = pred[0]['labels'], pred[0]['scores']\n",
    "    for i in range(labels.size()[-1]):\n",
    "        print(reverse_label_map[labels[i].item()], \"--\", scores[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topn_acc(model, ds, reverse_label_map, n=1, debug=True):\n",
    "    \n",
    "    results = np.zeros(len(ds))\n",
    "    \n",
    "    model.eval()        \n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(ds)):\n",
    "            image, target = ds[idx]        \n",
    "            pred = model([image.to(device)])                \n",
    "            \n",
    "            # sort predicitons by classifacation score\n",
    "            labels, scores = pred[0]['labels'], pred[0]['scores']\n",
    "            pred = list(zip(labels, scores))\n",
    "            pred.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            true_label = reverse_label_map[target['labels'][0].item()]\n",
    "            pred_labels = [reverse_label_map[l.item()] for l, s in pred]\n",
    "            if debug:\n",
    "                print(f\"true: {true_label}, pred: {[f'{reverse_label_map[l.item()]}: {s:.2f}' for l, s in pred]}\")\n",
    "            \n",
    "            if true_label in pred_labels[:n]:\n",
    "                results[idx] = 1\n",
    "                \n",
    "    return results            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "acc = topn_acc(loaded_model, ds_test, reverse_label_map, n=n, debug=False)\n",
    "print(f\"top{n}_acc: {acc.sum()/len(acc):.2f}, sample size: {len(ds_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c359592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need IOU for top prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e005d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# overall accuracy (warning - prints images)\n",
    "limit = 10\n",
    "randomise = True\n",
    "reverse_label_map = {v:k for k,v in label_map.items()}\n",
    "loaded_model.eval()\n",
    "results = []\n",
    "\n",
    "indices = list(range(len(ds_test)))\n",
    "if randomise:\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for idx in indices[:limit]:\n",
    "        image, target = ds_test[idx]        \n",
    "        pred = loaded_model([image.to(device)])    \n",
    "        display_annotation(image, target, label_map, prediction=pred[0], thresh=0.7, size=(10, 10))\n",
    "        \n",
    "        labels, scores = pred[0]['labels'], pred[0]['scores']\n",
    "        pred = list(zip(labels, scores))\n",
    "        pred.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"[{reverse_label_map[target['labels'][0].item()]}]\")\n",
    "        for l, s in pred:\n",
    "            print(reverse_label_map[l.item()], s.item())            \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f6152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
